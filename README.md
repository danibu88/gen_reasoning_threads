ReT-Eval: A prototype-inspired framework for generating effective reasoning threads by aligning user knowledge, domain-specific structures, and intrinsic LLM representations
This repository contains the complete implementation of ReT-Eval, a two-phase framework that leverages prototype-inspired reasoning to construct semantically coherent knowledge threads and evaluate effective reasoning paths for goal-oriented problem-solving. Our approach bridges the gap between user understanding and LLM knowledge through hierarchical knowledge graph construction and Monte Carlo Tree Search optimization.

ğŸ“– Paper
"Overcoming Knowledge Discrepancies: Structuring Reasoning Threads Through Knowledge Balancing in Interactive Scenarios"

ğŸ“„ Paper: arXiv:2408.xxxxx
ğŸ¯ Abstract: We introduce ReT-Eval, a two-phase framework that constructs semantically coherent knowledge threads and evaluates effective reasoning paths by integrating user knowledge threads (KU), domain-specific knowledge graphs (KGD), and intrinsic LLM representations (KL) through prototype-inspired reasoning and Monte Carlo Tree Search optimization.
ğŸ‘¨â€ğŸ’¼ Authors: Daniel Burkhardt

ğŸŒŸ Key Contributions
Two-Phase ReT-Eval Framework: Novel approach combining Prototype Construction (PC) and Effectiveness Evaluation (EE) phases for reasoning thread generation
Hierarchical Knowledge Integration: Systematic alignment of user knowledge (KU), domain-specific knowledge graphs (KGD), and LLM representations (KL)
Multi-Layer Domain Ontology: Structured knowledge representation across Business â†’ System â†’ Data â†’ Technology abstraction layers
MCTS-Based Thread Selection: Monte Carlo Tree Search with composite reward function R(Ï„) = Î±Â·Rsem(Ï„) + Î²Â·Ruser(Ï„) + Î³Â·Rdom(Ï„)
Comprehensive Multi-Dimensional Evaluation: Assessment across six effectiveness dimensions with both automated metrics and expert evaluation

ğŸ—ï¸ ReT-Eval Framework Architecture
ReT-Eval operates through a sophisticated two-phase pipeline:

Phase 1: Prototype Construction (PC)
User Knowledge Thread Extraction: Multi-stage NLP pipeline with NER, constituency parsing (Benepar), and triple extraction
KG Pruning: Semantic similarity-based filtering of domain knowledge graph using sentence transformers (757â†’140 nodes avg.)
LLM Enrichment: Extension of pruned subgraph with novel triples generated by LLM inference (~15 triples per subgraph)
GNN Traversal: Graph neural network exploration of enriched structure for semantic path discovery

Phase 2: Effectiveness Evaluation (EE)
Composite Reward Function: Multi-objective optimization balancing semantic coherence, user alignment, and domain progression
MCTS Optimization: Monte Carlo Tree Search with UCB scoring for reasoning thread selection
Hierarchical Progression: Guided traversal through Business â†’ System â†’ Data â†’ Technology layers

System Components
ğŸ¨ Frontend Interface (React): Interactive visualization of reasoning threads and knowledge graphs
ğŸ”§ Backend API (Flask/Python): Core ReT-Eval implementation with ML models and MCTS engine
ğŸ”— GraphQL Server (Node.js): Knowledge graph interface for Neo4j database operations
ğŸ” Elasticsearch: Document retrieval and article search for knowledge enrichment

ğŸš€ Quick Start
Prerequisites

Docker & Docker Compose
Git LFS (for large model files)
Python 3.8+ (for local development)
Node.js 16+ (for local development)

One-Command Setup
bash# Clone repository with LFS
git clone https://github.com/danibu88/gen_reasoning_threads.git
cd gen_reasoning-threads-paper

# Download models and setup data
./scripts/setup.sh

# Start entire system
docker-compose up
Access the System

Frontend: http://localhost:3000
Backend API: http://localhost:3002
GraphQL Playground: http://localhost:4001/graphql
Elasticsearch: http://localhost:9200

ğŸ“ Repository Structure
â”œâ”€â”€ ğŸ“„ paper/                          # Research paper and supplementary materials
â”œâ”€â”€ ğŸ’» src/                            # Complete source code
â”‚   â”œâ”€â”€ backend/                       # Flask API with ReT-Eval implementation
â”‚   â”œâ”€â”€ frontend/                      # React interface for reasoning visualization
â”‚   â””â”€â”€ graphql/                       # GraphQL server for Neo4j operations
â”œâ”€â”€ ğŸ“Š data/                           # Datasets and experimental data
â”‚   â”œâ”€â”€ raw/                          # Original domain-specific datasets
â”‚   â”œâ”€â”€ processed/                    # Preprocessed knowledge graphs
â”‚   â”œâ”€â”€ evaluation/                   # Evaluation datasets and benchmarks
â”‚   â””â”€â”€ elasticsearch/                # ES indices and import data
â”œâ”€â”€ ğŸ¤– models/                         # Machine learning models
â”‚   â”œâ”€â”€ triple_extraction/            # Word2Vec and Benepar models
â”‚   â”œâ”€â”€ embeddings/                   # MS MARCO DistilBERT fine-tuned
â”‚   â”œâ”€â”€ graph_neural_network/         # GNN for graph traversal
â”‚   â””â”€â”€ mcts/                         # MCTS policies and reward functions
â”œâ”€â”€ ğŸ—ï¸ infrastructure/                 # Deployment and service configuration
â”‚   â”œâ”€â”€ elasticsearch/               # ES setup and mappings
â”‚   â””â”€â”€ docker/                      # Docker configurations
â”œâ”€â”€ ğŸ› ï¸ scripts/                        # Setup and utility scripts
â”œâ”€â”€ ğŸ“š docs/                           # Detailed documentation
â””â”€â”€ ğŸ’¡ examples/                       # Usage examples and tutorials

ğŸ”¬ Reproducibility
Reproduce Paper Results
bash# Run complete ReT-Eval evaluation pipeline
./scripts/run_ret_eval_experiments.sh

# Generate specific results
python scripts/evaluate_mcts.py --config configs/mcts_eval.yaml
python scripts/evaluate_gnn.py --config configs/gnn_eval.yaml
python scripts/human_evaluation.py --config configs/human_eval.yaml
Key Experimental Results

MCTS Approach: Overall effectiveness score of 0.627 across six dimensions
GNN Method: Competitive performance with 0.611 overall effectiveness
Human Evaluation: MCTS achieves 4.1/5.0 average rating from domain experts

ğŸ“Š Datasets and Knowledge Base
Domain-Specific Knowledge Graph (KGD)

Structure: 757 entities, 1,374 relationships across hierarchical abstraction layers
Domains: Healthcare, mobility, logistics, finance, education, manufacturing
Hierarchy: Business â†’ System â†’ Data â†’ Technology abstraction layers
Prototypes: 10 domain-focused prototypes for cross-domain reasoning evaluation
Source: Extended from domain-specific ontology for data-driven solution design

Evaluation Datasets
Synthetic Prompt Dataset: ~700 domain triples for empirical testing
User Query Collection: Real-world queries across multiple technical domains
Expert Evaluation Set: Human assessment data from 7 PhD candidates
Ground Truth Annotations: Manually validated reasoning thread examples

Model Training Data
Triple Extraction Corpus: Domain-specific text for Word2Vec training
Graph Embeddings: Pre-trained on full domain knowledge graph
Sentence Transformers: MS MARCO DistilBERT fine-tuned for domain alignment

ğŸ¤– Models and Algorithms
Core ReT-Eval Components

Triple Extraction Pipeline: Multi-stage NLP with NER, constituency parsing (Benepar), and SVO extraction
Sentence Transformer: MS MARCO DistilBERT for semantic similarity and entity alignment
Graph Neural Network: Custom GNN architecture trained on domain knowledge graph
MCTS Engine: Monte Carlo Tree Search with composite reward function

Model Implementations
models/
â”œâ”€â”€ triple_extraction/
â”‚   â”œâ”€â”€ triples_ontology.model           # Word2Vec for domain-specific triple extraction (2.1GB)
â”‚   â””â”€â”€ benepar_constituency_parser/     # Constituency parsing models
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ msmarco_distilbert/              # Fine-tuned sentence transformer (400MB)
â”‚   â””â”€â”€ domain_embeddings.npy           # Pre-computed entity embeddings
â”œâ”€â”€ graph_neural_network/
â”‚   â”œâ”€â”€ gnn_model.pkl                    # Trained GNN for graph traversal (150MB)
â”‚   â””â”€â”€ node_embeddings.h5              # Contextualized node representations
â””â”€â”€ mcts/
    â”œâ”€â”€ reward_function.py               # Composite reward implementation
    â””â”€â”€ ucb_policy.pkl                   # UCB-based selection policy (80MB)
Algorithm Performance

Graph Pruning: Reduces 757 nodes to ~140 nodes on average (81% reduction)
LLM Enrichment: Generates ~15 novel triples per 145-node subgraph
GNN Traversal: Processes enriched graphs of ~139 nodes, 323 edges
MCTS Convergence: Optimal thread selection within K iterations

ğŸ¯ Usage Examples
Basic ReT-Eval Pipeline
pythonfrom src.backend.ret_eval import ReTEvalFramework

# Initialize ReT-Eval framework
ret_eval = ReTEvalFramework(
    domain_kg_path="data/domain_kg.json",
    gnn_model_path="models/graph_neural_network/gnn_model.pkl"
)

# Phase 1: Prototype Construction
user_input = "How does CNN improve automation in manufacturing?"
user_knowledge_thread = ret_eval.extract_user_knowledge(user_input)
pruned_subgraph = ret_eval.prune_kg_by_user_context(user_knowledge_thread)
enriched_graph = ret_eval.enrich_with_llm(pruned_subgraph)
knowledge_threads = ret_eval.gnn_traversal(enriched_graph)

# Phase 2: Effectiveness Evaluation
optimal_thread = ret_eval.mcts_thread_selection(
    knowledge_threads, 
    reward_weights={'alpha': 0.4, 'beta': 0.3, 'gamma': 0.3}
)

print(f"Optimal reasoning thread: {optimal_thread}")
print(f"Effectiveness score: {ret_eval.compute_reward(optimal_thread)}")
API Usage
bash# Generate reasoning thread via ReT-Eval API
curl -X POST http://localhost:3002/api/ret-eval/generate \
  -H "Content-Type: application/json" \
  -d '{
    "query": "CNN improves automation", 
    "domain": "manufacturing",
    "max_hops": 3,
    "mcts_iterations": 500
  }'
GraphQL Query for Knowledge Graph Operations
graphqlmutation GenerateReasoningThread($input: ReTEvalInput!) {
  generateReasoningThread(input: $input) {
    userKnowledgeThread {
      entities
      relations
      triples
    }
    prunedSubgraph {
      nodeCount
      edgeCount
      layers
    }
    enrichedGraph {
      llmGeneratedTriples
      contextualEnrichments
    }
    optimalThread {
      steps {
        layer
        concept
        evidence
        progression_score
      }
      effectiveness_score
      reward_breakdown {
        semantic_coherence
        user_alignment
        domain_progression
      }
    }
  }
}

ğŸ“ˆ Evaluation Framework
Multi-Dimensional Assessment
ReT-Eval is evaluated across six key effectiveness dimensions:

Actionability: Practical applicability and implementability of reasoning threads
Coherence: Logical consistency and semantic flow between reasoning steps
Domain Specificity: Relevance and accuracy within target domain contexts
Technical Specificity: Depth and precision of technical knowledge integration
Understandability: Clarity and accessibility for target user populations
User Focus: Alignment with user intent and contextual requirements

Baseline Comparison: Against GNN-only and Retrieval Methods (RM)
Cross-Domain Assessment: Evaluation across healthcare, manufacturing, finance, and education domains
Expert Panel: 7 PhD candidates rating reasoning thread quality and effectiveness
Ablation Studies: Component-wise analysis of PC and EE phases

ğŸ”§ Development Setup
Local Development
bash# Backend development
cd src/backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
dotenv -f .env.development run python app.py

# Frontend development
cd src/frontend
npm install
npx env-cmd -f .env.development npm start

# GraphQL development
cd src/graphql
npm install
npx env-cmd -f .env.development node graphql.js

# Adding New Models
bash# Add new model to the ReT-Eval pipeline
python scripts/train_new_model.py --config configs/new_model.yaml
python scripts/evaluate_model.py --model models/new_model.pkl

# Other things to consider

ğŸ¤ Contributing
We welcome contributions! Please see our Contributing Guide for details.
Areas for Contribution

Model Improvements: Enhanced MCTS algorithms and reward functions
Domain Expansion: New domain-specific adaptations and ontologies
Evaluation Metrics: Additional effectiveness measures and benchmarks
User Interface: Enhanced visualization and interaction for reasoning threads
Performance: Optimization and scalability improvements for large knowledge graphs

ğŸ“„ Citation
If you use this work in your research, please cite:
bibtex@misc{burkhardt2025reteval,
  title={Overcoming Knowledge Discrepancies: Structuring Reasoning Threads Through Knowledge Balancing in Interactive Scenarios},
  author={Burkhardt, Daniel},
  year={2025},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2408.xxxxx}
}

ğŸ“œ License
This project is licensed under the MIT License - see the [LICENSE](https://www.mit.edu/~amini/LICENSE.md) file for details.

ğŸ™ Acknowledgments
Neo4j Community: For graph database infrastructure
Hugging Face: For transformer model foundations
Elasticsearch: For search and retrieval capabilities
Research Community: For valuable feedback and collaboration
PhD Evaluation Panel: For comprehensive human evaluation of reasoning threads

ğŸ”¬ For researchers: This implementation provides a complete framework for reasoning thread generation research with the ReT-Eval methodology.
ğŸ­ For practitioners: The system offers production-ready components for knowledge-aware applications with hierarchical reasoning capabilities.
